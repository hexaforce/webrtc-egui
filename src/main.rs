#![cfg_attr(target_os = "macos", allow(unexpected_cfgs))]

use eframe::egui;
use gst::prelude::*;
use std::sync::{Arc, Mutex};
use anyhow::Error;

/// WebRTCå—ä¿¡ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã®çŠ¶æ…‹ã‚’ç®¡ç†ã™ã‚‹æ§‹é€ ä½“
struct WebRtcApp {
    pipeline: Option<gst::Pipeline>,
    logs: Arc<Mutex<Vec<String>>>,
    is_running: bool,
    video_texture: Option<egui::TextureHandle>,
    video_frame: Arc<Mutex<Option<VideoFrame>>>,
}

/// ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æŒã™ã‚‹æ§‹é€ ä½“
struct VideoFrame {
    width: usize,
    height: usize,
    data: Vec<u8>,
}

impl Default for WebRtcApp {
    fn default() -> Self {
        #[cfg(target_os = "macos")]
        init_macos_app();

        // GStreamerã®åˆæœŸåŒ–
        if let Err(e) = gst::init() {
            eprintln!("Failed to initialize GStreamer: {}", e);
        }

        Self {
            pipeline: None,
            logs: Arc::new(Mutex::new(Vec::new())),
            is_running: false,
            video_texture: None,
            video_frame: Arc::new(Mutex::new(None)),
        }
    }
}

impl WebRtcApp {
    fn add_log(&self, message: String) {
        if let Ok(mut logs) = self.logs.lock() {
            logs.push(message);
            // æœ€æ–°100ä»¶ã®ã¿ä¿æŒ
            if logs.len() > 100 {
                logs.remove(0);
            }
        }
    }

    fn start_pipeline(&mut self) -> Result<(), Error> {
        if self.is_running {
            return Ok(());
        }

        let pipeline = gst::Pipeline::builder().build();

        // webrtcsrcã®ä½œæˆ - ä½é…å»¶è¨­å®š
        let webrtcsrc = gst::ElementFactory::make("webrtcsrc")
            .property("connect-to-first-producer", true)
            .property_from_str("video-codecs", "<H264, VP8>")
            .property_from_str("audio-codecs", "<OPUS>")
            .property("enable-control-data-channel", true)
            .build()?;

        pipeline.add(&webrtcsrc)?;

        let signaller = webrtcsrc.property::<gst::glib::Object>("signaller");

        // ãƒ­ã‚°ç”¨ã®ã‚¯ãƒ­ãƒ¼ã‚¸ãƒ£
        let logs = self.logs.clone();
        signaller.connect("producer-added", false, move |args| {
            let producer_id = args[1].get::<String>().unwrap();
            let meta = args[2].get::<Option<gst::Structure>>().unwrap();
            if let Ok(mut logs) = logs.lock() {
                logs.push(format!("ğŸ¤ Producerè¿½åŠ : producer_id={}, meta={:?}", producer_id, meta));
            }
            None
        });

        let logs = self.logs.clone();
        signaller.connect("session-requested", false, move |args| {
            let session_id = args[1].get::<String>().unwrap();
            let peer_id = args[2].get::<String>().unwrap();
            if let Ok(mut logs) = logs.lock() {
                logs.push(format!("ğŸ“ ã‚»ãƒƒã‚·ãƒ§ãƒ³è¦æ±‚: peer_id={}, session_id={}", peer_id, session_id));
            }
            None
        });

        let logs = self.logs.clone();
        signaller.connect("session-started", false, move |args| {
            let session_id = args[1].get::<String>().unwrap();
            let peer_id = args[2].get::<String>().unwrap();
            if let Ok(mut logs) = logs.lock() {
                logs.push(format!("âœ… ã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹: peer_id={}, session_id={}", peer_id, session_id));
            }
            None
        });

        let logs = self.logs.clone();
        signaller.connect("webrtcbin-ready", false, move |args| {
            let webrtcbin = args[2].get::<gst::Element>().unwrap();
            webrtcbin.set_property("latency", 20u32);
            if let Ok(mut logs) = logs.lock() {
                logs.push("ğŸ¬ WebRTCBin ready - ä½é…å»¶è¨­å®šã‚’é©ç”¨ã—ã¾ã—ãŸ".to_string());
            }
            None
        });

        // pad-addedã‚·ã‚°ãƒŠãƒ«: videoã¨audioã®ãƒ‘ãƒƒãƒ‰ã‚’å‹•çš„ã«æ¥ç¶š
        let video_frame = self.video_frame.clone();
        let logs_for_pad = self.logs.clone();
        webrtcsrc.connect_pad_added(move |webrtcsrc, pad| {
            let Some(pipeline) = webrtcsrc
                .parent()
                .and_then(|p| p.downcast::<gst::Pipeline>().ok())
            else {
                return;
            };

            if pad.name().starts_with("audio") {
                if let Ok(mut logs) = logs_for_pad.lock() {
                    logs.push("ğŸ”Š Audio padè¿½åŠ ".to_string());
                }

                let audioconvert = gst::ElementFactory::make("audioconvert").build().unwrap();
                let audioresample = gst::ElementFactory::make("audioresample").build().unwrap();
                let queue = gst::ElementFactory::make("queue")
                    .property("max-size-buffers", 1u32)
                    .property("max-size-bytes", 0u32)
                    .property("max-size-time", 0u64)
                    .build()
                    .unwrap();
                let audiosink = gst::ElementFactory::make("autoaudiosink")
                    .build()
                    .unwrap();

                pipeline.add_many([&audioconvert, &audioresample, &queue, &audiosink]).unwrap();
                pad.link(&audioconvert.static_pad("sink").unwrap()).unwrap();
                gst::Element::link_many([&audioconvert, &audioresample, &queue, &audiosink]).unwrap();

                audiosink.sync_state_with_parent().unwrap();
                queue.sync_state_with_parent().unwrap();
                audioresample.sync_state_with_parent().unwrap();
                audioconvert.sync_state_with_parent().unwrap();
            } else if pad.name().starts_with("video") {
                if let Ok(mut logs) = logs_for_pad.lock() {
                    logs.push("ğŸ¥ Video padè¿½åŠ ".to_string());
                }

                let videoconvert = gst::ElementFactory::make("videoconvert").build().unwrap();
                let videoscale = gst::ElementFactory::make("videoscale").build().unwrap();
                let queue = gst::ElementFactory::make("queue")
                    .property("max-size-buffers", 1u32)
                    .property("max-size-bytes", 0u32)
                    .property("max-size-time", 0u64)
                    .build()
                    .unwrap();

                // appsinkã‚’ä½¿ç”¨ã—ã¦ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’ã‚­ãƒ£ãƒ—ãƒãƒ£
                let appsink = gst_app::AppSink::builder()
                    .caps(
                        &gst::Caps::builder("video/x-raw")
                            .field("format", "RGBA")
                            .build()
                    )
                    .build();

                let video_frame_clone = video_frame.clone();
                appsink.set_callbacks(
                    gst_app::AppSinkCallbacks::builder()
                        .new_sample(move |appsink| {
                            let sample = appsink.pull_sample().map_err(|_| gst::FlowError::Eos)?;
                            let buffer = sample.buffer().ok_or(gst::FlowError::Error)?;
                            let caps = sample.caps().ok_or(gst::FlowError::Error)?;
                            
                            let video_info = gst_video::VideoInfo::from_caps(caps)
                                .map_err(|_| gst::FlowError::Error)?;
                            
                            let map = buffer.map_readable().map_err(|_| gst::FlowError::Error)?;
                            
                            if let Ok(mut frame) = video_frame_clone.lock() {
                                *frame = Some(VideoFrame {
                                    width: video_info.width() as usize,
                                    height: video_info.height() as usize,
                                    data: map.as_slice().to_vec(),
                                });
                            }
                            
                            Ok(gst::FlowSuccess::Ok)
                        })
                        .build()
                );

                pipeline.add_many([&videoconvert, &videoscale, &queue, appsink.upcast_ref()]).unwrap();
                pad.link(&videoconvert.static_pad("sink").unwrap()).unwrap();
                gst::Element::link_many([&videoconvert, &videoscale, &queue, appsink.upcast_ref()]).unwrap();

                appsink.sync_state_with_parent().unwrap();
                queue.sync_state_with_parent().unwrap();
                videoscale.sync_state_with_parent().unwrap();
                videoconvert.sync_state_with_parent().unwrap();
            }
        });

        // ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³èµ·å‹•
        pipeline.set_state(gst::State::Playing)?;

        // ãƒã‚¹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸å‡¦ç†ç”¨ã®ã‚¹ãƒ¬ãƒƒãƒ‰ã‚’èµ·å‹•
        let bus = pipeline.bus().expect("Pipeline should have a bus");
        let pipeline_weak = pipeline.downgrade();
        let logs = self.logs.clone();
        
        std::thread::spawn(move || {
            for msg in bus.iter_timed(gst::ClockTime::NONE) {
                use gst::MessageView;
                
                match msg.view() {
                    MessageView::Eos(..) => {
                        if let Ok(mut logs) = logs.lock() {
                            logs.push("â¹ï¸ EOS".to_string());
                        }
                        break;
                    }
                    MessageView::Error(err) => {
                        if let Ok(mut logs) = logs.lock() {
                            logs.push(format!("âŒ Error: {}", err.error()));
                        }
                        if let Some(pipeline) = pipeline_weak.upgrade() {
                            let _ = pipeline.set_state(gst::State::Null);
                        }
                        break;
                    }
                    MessageView::Latency(_) => {
                        if let Some(pipeline) = pipeline_weak.upgrade() {
                            let _ = pipeline.recalculate_latency();
                        }
                    }
                    _ => (),
                }
            }
        });

        self.pipeline = Some(pipeline);
        self.is_running = true;
        self.add_log("â–¶ï¸ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹".to_string());

        Ok(())
    }

    fn stop_pipeline(&mut self) {
        if let Some(pipeline) = self.pipeline.take() {
            let _ = pipeline.set_state(gst::State::Null);
            self.is_running = false;
            self.add_log("â¹ï¸ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åœæ­¢".to_string());
        }
    }
}

impl eframe::App for WebRtcApp {
    fn update(&mut self, ctx: &egui::Context, _frame: &mut eframe::Frame) {
        // å®šæœŸçš„ã«å†æç”»ã‚’ãƒªã‚¯ã‚¨ã‚¹ãƒˆ
        ctx.request_repaint();

        egui::CentralPanel::default().show(ctx, |ui| {
            ui.heading("WebRTC ä½é…å»¶å—ä¿¡ GUI");

            ui.horizontal(|ui| {
                if ui.button(if self.is_running { "â¹ï¸ åœæ­¢" } else { "â–¶ï¸ é–‹å§‹" }).clicked() {
                    if self.is_running {
                        self.stop_pipeline();
                    } else {
                        if let Err(e) = self.start_pipeline() {
                            self.add_log(format!("âŒ ã‚¨ãƒ©ãƒ¼: {}", e));
                        }
                    }
                }

                ui.label(if self.is_running { "ğŸŸ¢ å®Ÿè¡Œä¸­" } else { "ğŸ”´ åœæ­¢ä¸­" });
            });

            ui.separator();

            // ãƒ“ãƒ‡ã‚ªè¡¨ç¤ºã‚¨ãƒªã‚¢
            ui.heading("ãƒ“ãƒ‡ã‚ª");
            
            if let Ok(frame_guard) = self.video_frame.lock() {
                if let Some(frame) = frame_guard.as_ref() {
                    let color_image = egui::ColorImage::from_rgba_unmultiplied(
                        [frame.width, frame.height],
                        &frame.data,
                    );
                    
                    let texture = ctx.load_texture(
                        "video-frame",
                        color_image,
                        egui::TextureOptions::LINEAR,
                    );
                    
                    ui.image(&texture);
                    self.video_texture = Some(texture);
                } else {
                    ui.label("ãƒ“ãƒ‡ã‚ªãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å¾…æ©Ÿä¸­...");
                }
            }

            ui.separator();

            // ãƒ­ã‚°è¡¨ç¤ºã‚¨ãƒªã‚¢
            ui.heading("ãƒ­ã‚°");
            egui::ScrollArea::vertical()
                .max_height(200.0)
                .stick_to_bottom(true)
                .show(ui, |ui| {
                    if let Ok(logs) = self.logs.lock() {
                        for log in logs.iter() {
                            ui.label(log);
                        }
                    }
                });
        });
    }

    fn on_exit(&mut self, _gl: Option<&eframe::glow::Context>) {
        self.stop_pipeline();
    }
}

fn main() -> Result<(), eframe::Error> {
    let options = eframe::NativeOptions {
        viewport: egui::ViewportBuilder::default()
            .with_inner_size([800.0, 600.0])
            .with_title("WebRTC ä½é…å»¶å—ä¿¡"),
        ..Default::default()
    };

    eframe::run_native(
        "webrtc-egui",
        options,
        Box::new(|cc| {
            // IBM Plex Sans JP ãƒ•ã‚©ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã‚“ã§è¨­å®š
            let mut fonts = egui::FontDefinitions::default();

            // ãƒ•ã‚©ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
            fonts.font_data.insert(
                "ibm_plex_sans_jp".to_owned(),
                egui::FontData::from_static(include_bytes!("../fonts/IBMPlexSansJP-Regular.ttf")).into(),
            );

            // Proportionalï¼ˆãƒ—ãƒ­ãƒãƒ¼ã‚·ãƒ§ãƒŠãƒ«ï¼‰ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦è¨­å®šï¼ˆå„ªå…ˆåº¦æœ€é«˜ï¼‰
            fonts.families
                .entry(egui::FontFamily::Proportional)
                .or_default()
                .insert(0, "ibm_plex_sans_jp".to_owned());

            // Monospaceï¼ˆç­‰å¹…ï¼‰ãƒ•ã‚©ãƒ³ãƒˆã¨ã—ã¦ã‚‚è¨­å®š
            fonts.families
                .entry(egui::FontFamily::Monospace)
                .or_default()
                .insert(0, "ibm_plex_sans_jp".to_owned());

            // ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã‚’é©ç”¨
            cc.egui_ctx.set_fonts(fonts);

            Ok(Box::new(WebRtcApp::default()))
        }),
    )
}

#[cfg(target_os = "macos")]
#[allow(unexpected_cfgs)]
fn init_macos_app() {
    use objc::{class, msg_send, sel, sel_impl};
    unsafe {
        let ns_app: *mut objc::runtime::Object = msg_send![class!(NSApplication), sharedApplication];
        let _: () = msg_send![ns_app, finishLaunching];
    }
}
